{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOpxaJ14Ncpcfi3uAXOWdeY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pHlxhLPYopDm"},"source":["# Libraries\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import torch\n","# Preliminaries\n","from torchtext.legacy import data\n","# Models\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","# Training\n","import torch.optim as optim\n","# Evaluation\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iRQo3gK4kmj","executionInfo":{"status":"ok","timestamp":1634647459555,"user_tz":-480,"elapsed":22322,"user":{"displayName":"D T","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08867414033985456014"}},"outputId":"219883e1-4688-460c-d395-6b142f5e1b84"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dpOd7se4m-F","executionInfo":{"status":"ok","timestamp":1634647459556,"user_tz":-480,"elapsed":15,"user":{"displayName":"D T","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08867414033985456014"}},"outputId":"37f9ce18-7cfc-4cf5-c381-4f41087b9d2d"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Oct 19 12:44:19 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   52C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"dp4z7VaXLIMS"},"source":["# Set Random Seed"]},{"cell_type":"code","metadata":{"id":"rn-RP01Epz4i"},"source":["#Reproducing same results\n","SEED = 2021\n","#Torch\n","torch.manual_seed(SEED)\n","#Cuda algorithms\n","torch.backends.cudnn.deterministic = True  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMQYi-NiLSRR"},"source":["# Change Your Path Here"]},{"cell_type":"code","metadata":{"id":"EMNpMMxnpULP"},"source":["root_dir = 'drive/MyDrive/your/root/path/'\n","data_dir = root_dir + 'Case Presentation 1 Data/'\n","#check whether cuda is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","#set batch size\n","BATCH_SIZE = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNBDm17Yo2ay"},"source":["# Fields\n","text_field = data.Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\n","label_field = data.Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n","fields = [('text', text_field), ('label', label_field)]\n","# TabularDataset\n","train, valid, test = data.TabularDataset.splits(path=data_dir, train='train.csv', validation='test.csv', test='valid.csv', format='CSV', fields=fields, skip_header=True)\n","# Iterators\n","train_iterator = data.BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n","                            device=device, sort=True, sort_within_batch=True)\n","valid_iterator = data.BucketIterator(valid, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n","                            device=device, sort=True, sort_within_batch=True)\n","test_iterator = data.BucketIterator(test, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n","                            device=device, sort=True, sort_within_batch=True)\n","# Vocabulary\n","text_field.build_vocab(train, min_freq=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJCU6kZ1LEjl"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"4q7iqMTutWTW"},"source":["class classifier(nn.Module):\n","    \n","  #define all the layers used in model\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n","               bidirectional, dropout):\n","    #Constructor\n","    super().__init__()          \n","    \n","    #embedding layer\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    \n","    #lstm layer\n","    self.lstm = nn.LSTM(embedding_dim,\n","                        hidden_dim, \n","                        num_layers=n_layers, \n","                        bidirectional=bidirectional, \n","                        dropout=dropout,\n","                        batch_first=True)\n","    \n","    #dense layer\n","    self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","    \n","    #activation function\n","    self.act = nn.Sigmoid()\n","\n","  def forward(self, text, text_lengths):\n","\n","    #text = [batch size,sent_length]\n","    embedded = self.embedding(text)\n","    #embedded = [batch size, sent_len, emb dim]\n","  \n","    #packed sequence\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n","    \n","    packed_output, (hidden, cell) = self.lstm(packed_embedded)\n","    #hidden = [batch size, num layers * num directions,hid dim]\n","    #cell = [batch size, num layers * num directions,hid dim]\n","    \n","    #concat the final forward and backward hidden state\n","    hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n","            \n","    #hidden = [batch size, hid dim * num directions]\n","    dense_outputs=self.fc(hidden)\n","\n","    #Final activation function\n","    outputs=self.act(dense_outputs)\n","    \n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m60Sb6jYt1DB"},"source":["#define hyperparameters\n","size_of_vocab = len(text_field.vocab)\n","embedding_dim = 100\n","num_hidden_nodes = 32\n","num_output_nodes = 1\n","num_layers = 2\n","bidirection = True\n","dropout = 0.2\n","\n","#instantiate the model\n","model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers,\n","                   bidirectional=True, dropout=dropout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b9vDhn1ZuNCV"},"source":["#architecture\n","print(model)\n","\n","#No. of trianable parameters\n","def count_parameters(model):\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    \n","print('The model has {} trainable parameters'.format(count_parameters(model)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PNmZX49yK5Gw"},"source":["# Loss & Optimizer"]},{"cell_type":"code","metadata":{"id":"sK_tqxSjumhD"},"source":["import torch.optim as optim\n","\n","#define optimizer and loss\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.BCELoss()\n","\n","#define metric\n","def binary_accuracy(preds, y):\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(preds)\n","    \n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum() / len(correct)\n","    return acc\n","    \n","#push to cuda if available\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vo3aAxUAK1zW"},"source":["# TRAIN"]},{"cell_type":"code","metadata":{"id":"wIUDlWsku3ds"},"source":["def train(model, iterator, optimizer, criterion):\n","  #initialize every epoch\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  #set the model in training phase\n","  model.train()\n","\n","  for batch in iterator:\n","    #resets the gradients after every batch\n","    optimizer.zero_grad()\n","\n","    #retrieve text and no. of words\n","    text, text_lengths = batch.text\n","\n","    #convert to 1D tensor\n","    predictions = model(text, text_lengths).squeeze()\n","\n","    #compute the loss\n","    loss = criterion(predictions, batch.label)\n","\n","    #compute the binary accuracy\n","    acc = binary_accuracy(predictions, batch.label)\n","\n","    #backpropage the loss and compute the gradients\n","    loss.backward()\n","\n","    #update the weights\n","    optimizer.step()\n","\n","    #loss and accuracy\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDfIc33DvUIg"},"source":["def evaluate(model, iterator, criterion):\n","  #initialize every epoch\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  #deactivating dropout layers\n","  model.eval()\n","\n","  #deactivates autograd\n","  with torch.no_grad():\n","    for batch in iterator:\n","      #retrieve text and no. of words\n","      text, text_lengths = batch.text\n","\n","      #convert to 1d tensor\n","      predictions = model(text, text_lengths).squeeze()\n","\n","      #compute loss and accuracy\n","      loss = criterion(predictions, batch.label)\n","      acc = binary_accuracy(predictions, batch.label)\n","\n","      #keep track of loss and accuracy\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-zcvmC5xaMB"},"source":["# Save and Load Functions\n","def save_checkpoint(save_path, model, optimizer, valid_loss):\n","  if save_path == None:\n","      return\n","\n","  state_dict = {'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'valid_loss': valid_loss}\n","\n","  torch.save(state_dict, save_path)\n","  print(f'Model saved to ==> {save_path}')\n","\n","def load_checkpoint(load_path, model, optimizer):\n","\n","  if load_path==None:\n","      return\n","\n","  state_dict = torch.load(load_path, map_location=device)\n","  print(f'Model loaded from <== {load_path}')\n","\n","  model.load_state_dict(state_dict['model_state_dict'])\n","  optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n","\n","  return state_dict['valid_loss']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEFnv02iv-Ho"},"source":["N_EPOCHS = 50\n","ckpt_dir = root_dir + 'ckpt/'\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","  print('Epoch: {}'.format(epoch+1))\n","  #train the model\n","  train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","\n","  #evaluate the model\n","  valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","\n","  #save the best model\n","  if valid_loss < best_valid_loss:\n","    best_valid_loss = valid_loss\n","    #save_checkpoint(ckpt_dir + 'weights.pt', model, optimizer, best_valid_loss)\n","    save_checkpoint(ckpt_dir + 'weights.pt', model, optimizer, best_valid_loss)\n","\n","  print('\\tTrain Loss: {:.3f} | Train Acc: {:.2f}%'.format(train_loss*100, train_acc*100))\n","  print('\\t Val. Loss: {:.3f} |  Val. Acc: {:.2f}%'.format(valid_loss*100, valid_acc*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAM90xnKKxU2"},"source":["# TEST"]},{"cell_type":"code","metadata":{"id":"tre8dgIdU36T"},"source":["#instantiate the model\n","test_model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers,\n","                   bidirectional=True, dropout=dropout)\n","#load weights\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","#load_checkpoint(ckpt_dir + 'weights.pt', test_model, optimizer)\n","load_checkpoint(ckpt_dir + 'weights.pt', test_model, optimizer)\n","test_model = test_model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5gIlz8uydQp"},"source":["def test(model, iterator):\n","  model.eval();\n","\n","  with torch.no_grad():\n","    ID, pred = [], []\n","    for batch in iterator:\n","      text, text_lengths = batch.text\n","\n","      predictions = model(text, text_lengths).squeeze()\n","      predictions = torch.round(predictions)\n","      predictions = predictions.cpu().numpy()\n","\n","      for i in batch.label:\n","        id = i.cpu().numpy()\n","        id = int(id)\n","        id = 'ID_' + str(id) +'.txt'\n","        ID.append(id)\n","      for i in predictions:\n","        i = int(i)\n","        if i == 1:\n","          pred.append(0)\n","        else:\n","          pred.append(1)\n","  return ID, pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yv-cGvWeDzo_"},"source":["test_ID, test_pred = test(test_model, test_iterator)\n","\n","temp = list(zip(test_ID, test_pred))\n","temp.sort()\n","test_ID, test_pred = zip(*temp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hiav1TJ_EFpb"},"source":["def LISTtoCSV(id, pred, dir_path, filename):\n","  data_dict = {\"Filename\": id, \"Obesity\": pred}\n","\n","  # 轉為dataframe再透過pandas轉成csv檔\n","  result_df = pd.DataFrame(data_dict)\n","  result_df.to_csv(dir_path+'{}.csv'.format(filename), index=False)\n","  print(result_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BtsSLxlYFmiG"},"source":["LISTtoCSV(test_ID, test_pred, root_dir, 'result')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CJF2zpHWKqh9"},"source":["# Draw Confusion Matrix"]},{"cell_type":"code","metadata":{"id":"DZDMuS1MguU1"},"source":["def draw_confusion_matrix(model, iterator):\n","  model.eval()\n","\n","  with torch.no_grad():\n","    pred, label = [], []\n","    for batch in iterator:\n","      text, text_lengths = batch.text\n","\n","      predictions = model(text, text_lengths).squeeze()\n","      predictions = torch.round(predictions)\n","\n","      pred.extend(predictions.tolist())\n","      label.extend(batch.label.tolist())\n","\n","    print('Classification Report:')\n","    print(classification_report(label, pred, labels=[1,0], digits=4))\n","    \n","    cm = confusion_matrix(label, pred, labels=[1,0])\n","    ax = plt.subplot()\n","    sns.heatmap(cm, annot=True, ax=ax, cmap='Blues', fmt=\"d\")\n","\n","    ax.set_title('Confusion Matrix')\n","\n","    ax.set_xlabel('Predictions')\n","    ax.set_ylabel('Ground-Truth')\n","\n","    ax.xaxis.set_ticklabels(['NO', 'YES'])\n","    ax.yaxis.set_ticklabels(['NO', 'YES'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2e6x06Qk1He"},"source":["draw_confusion_matrix(test_model, valid_iterator)"],"execution_count":null,"outputs":[]}]}